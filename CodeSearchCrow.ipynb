{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPFsI74mmlmj9jpnvjHkX7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shun0212/CodeSearch-Crow/blob/main/CodeCrow_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install flash-attn lizard faiss-cpu"
      ],
      "metadata": {
        "id": "rDIz1f3SUV7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bITKgPsCT_yd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import lizard\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ===========================================\n",
        "# Settings\n",
        "# ===========================================\n",
        "GITHUB_REPO_URL = \"https://github.com/google-research/bert.git\"  # üîß Change to any GitHub repo you want\n",
        "MODEL_NAME = \"Shuu12121/CodeSearch-ModernBERT-Crow-Plus\"\n",
        "MIN_FUNCTION_LENGTH = 3  # Only include functions/cells with 3+ lines\n",
        "SAVE_DIR = \"./cloned_repos\"\n",
        "\n",
        "# „Éï„Ç°„Ç§„É´ÂêçÂÆöÁæ©\n",
        "FUNCTIONS_FILE = \"functions.json\"\n",
        "INDEX_FILE = \"faiss_index.bin\"\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# Helper Functions\n",
        "# ===========================================\n",
        "\n",
        "def clone_repository(repo_url, clone_dir):\n",
        "    \"\"\"\n",
        "    Clone the GitHub repository if not already cloned.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(clone_dir):\n",
        "        subprocess.run([\"git\", \"clone\", repo_url, clone_dir], check=True)\n",
        "        print(f\"‚úÖ Repository cloned to {clone_dir}\")\n",
        "    else:\n",
        "        print(f\"‚ÑπÔ∏è Repository already exists at {clone_dir}. Skipping clone.\")\n",
        "\n",
        "\n",
        "def extract_functions(repo_path):\n",
        "    \"\"\"\n",
        "    Extract functions from .py and .ipynb files.\n",
        "    Uses lizard's long_name to include class names if available.\n",
        "    \"\"\"\n",
        "    functions = []\n",
        "    print(\"üì• Extracting functions...\")\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        # .git„Å™„Å©„ÅÆÈö†„Åó„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇÑ‰∏çË¶Å„Å™„Éï„Ç°„Ç§„É´„ÅØ„Çπ„Ç≠„ÉÉ„Éó (ÂâçÂõû„ÅÆ‰øÆÊ≠£„ÅßËøΩÂä†„Åó„ÅüË¶ÅÁ¥†)\n",
        "        if \".git\" in root or \".ipynb_checkpoints\" in root:\n",
        "             continue\n",
        "\n",
        "        files.sort()  # Sort files for stable order\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                if file.endswith(\".py\"):\n",
        "                    analysis = lizard.analyze_file(file_path)\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        lines = f.readlines()\n",
        "                    for func in analysis.function_list:\n",
        "                        if hasattr(func, 'start_line') and hasattr(func, 'end_line'):\n",
        "                            start, end = max(func.start_line - 1, 0), func.end_line\n",
        "                            code = \"\".join(lines[start:end]) # „Ç≥„Éº„Éâ„ÇíÊñáÂ≠óÂàó„Å®„Åó„Å¶ÁµêÂêà\n",
        "                            if len(code.strip().splitlines()) >= MIN_FUNCTION_LENGTH:\n",
        "                                functions.append({\n",
        "                                    \"file_path\": file_path,\n",
        "                                    \"function_name\": func.long_name,  # Use long_name (with class if exists)\n",
        "                                    \"code\": code\n",
        "                                })\n",
        "                elif file.endswith(\".ipynb\"):\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        data = json.load(f)\n",
        "                    for idx, cell in enumerate(data.get(\"cells\", [])):\n",
        "                        if cell.get(\"cell_type\") == \"code\":\n",
        "                            code = \"\".join(cell.get(\"source\", []))\n",
        "                            if len(code.strip().splitlines()) >= MIN_FUNCTION_LENGTH:\n",
        "                                functions.append({\n",
        "                                    \"file_path\": file_path,\n",
        "                                    \"function_name\": f\"cell_{idx}\",\n",
        "                                    \"code\": code\n",
        "                                })\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Warning: Could not process {file_path}: {e}\")\n",
        "\n",
        "    print(f\"‚úÖ Extracted {len(functions)} functions.\")\n",
        "    return functions\n",
        "\n",
        "\n",
        "def embed_codes(codes, model):\n",
        "    \"\"\"\n",
        "    Embed code snippets into dense vectors.\n",
        "    \"\"\"\n",
        "    print(\"\\nüìà Encoding function codes...\")\n",
        "    return model.encode(codes, batch_size=32, show_progress_bar=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def build_faiss_index(embeddings):\n",
        "    \"\"\"\n",
        "    Build a FAISS index from embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\nüèóÔ∏è Building FAISS index...\")\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "\n",
        "def load_or_build_data_and_index(clone_path, model):\n",
        "    \"\"\"\n",
        "    Load existing functions data and FAISS index, or build them if not found.\n",
        "    \"\"\"\n",
        "    functions_path = os.path.join(clone_path, FUNCTIONS_FILE)\n",
        "    index_path = os.path.join(clone_path, INDEX_FILE)\n",
        "\n",
        "    # Check if both data and index files exist\n",
        "    if os.path.exists(functions_path) and os.path.exists(index_path):\n",
        "        print(f\"\\nüîÑ Loading existing data and index from {clone_path}...\")\n",
        "        try:\n",
        "            # Load functions data\n",
        "            with open(functions_path, 'r', encoding='utf-8') as f:\n",
        "                functions = json.load(f)\n",
        "            # Load FAISS index\n",
        "            index = faiss.read_index(index_path)\n",
        "            print(\"‚úÖ Successfully loaded existing data and index.\")\n",
        "            return functions, index\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error loading existing data or index: {e}. Rebuilding...\")\n",
        "            # If loading fails, proceed to rebuild\n",
        "\n",
        "    # If data or index files do not exist, or loading failed, build them\n",
        "    print(f\"\\nüèóÔ∏è No existing data or index found (or failed to load). Building a new one...\")\n",
        "\n",
        "    # Extract functions\n",
        "    functions = extract_functions(clone_path)\n",
        "    if not functions:\n",
        "        print(\"‚ùå Error: No functions found. Cannot build index. Exiting.\")\n",
        "        return [], None # Return empty list and None for main to handle\n",
        "\n",
        "    # Save functions data\n",
        "    try:\n",
        "        with open(functions_path, 'w', encoding='utf-8') as f:\n",
        "            # json.dump„ÅØ„Éá„Éï„Ç©„É´„Éà„ÅßÈùûASCIIÊñáÂ≠ó„Çí„Ç®„Çπ„Ç±„Éº„Éó„Åô„Çã„ÅÆ„Åß„ÄÅensure_ascii=False„ÅßÊó•Êú¨Ë™û„Å™„Å©„Çí„Åù„ÅÆ„Åæ„Åæ‰øùÂ≠ò\n",
        "            json.dump(functions, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"üíæ Functions data saved at: {functions_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not save functions data to {functions_path}: {e}\")\n",
        "\n",
        "    # Embed codes\n",
        "    codes = [func[\"code\"] for func in functions]\n",
        "    embeddings = embed_codes(codes, model)\n",
        "\n",
        "    # Build FAISS index\n",
        "    index = build_faiss_index(embeddings)\n",
        "\n",
        "    # Save FAISS index\n",
        "    try:\n",
        "        faiss.write_index(index, index_path)\n",
        "        print(f\"üíæ FAISS index saved at: {index_path}\")\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Warning: Could not save FAISS index to {index_path}: {e}\")\n",
        "\n",
        "    return functions, index\n",
        "\n",
        "\n",
        "def search_functions(index, model, query, functions, top_k=5):\n",
        "    \"\"\"\n",
        "    Search for top-k most relevant functions given a natural language query.\n",
        "    \"\"\"\n",
        "    # Check if the model has a device attribute, if not, default to cpu\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if hasattr(model, 'device'):\n",
        "         device = model.device\n",
        "\n",
        "    query_emb = model.encode([query], device=device)\n",
        "\n",
        "    D, I = index.search(np.array(query_emb).astype('float32'), top_k) # Embeddings might need float32\n",
        "\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "         if 0 <= idx < len(functions): # Check if the index is within bounds\n",
        "              results.append(functions[idx])\n",
        "         else:\n",
        "              print(f\"‚ö†Ô∏è Warning: Invalid index {idx} returned from FAISS search. Skipping result.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def pretty_print_results(results):\n",
        "    \"\"\"\n",
        "    Display search results in a clean format.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç Search Results:\")\n",
        "    if not results:\n",
        "        print(\"No relevant functions found.\")\n",
        "        return\n",
        "\n",
        "    for idx, res in enumerate(results, start=1):\n",
        "        print(f\"\\n=== Result {idx} ===\")\n",
        "        print(f\"üìÑ File: {res['file_path']}\")\n",
        "        print(f\"üîß Function: {res['function_name']}\")\n",
        "        print(f\"üß© Code Preview:\")\n",
        "        lines = res['code'].splitlines()\n",
        "        # Limit preview lines\n",
        "        preview_lines = 100\n",
        "        for line in lines[:preview_lines]:\n",
        "            print(line)\n",
        "        if len(lines) > preview_lines:\n",
        "            print(f\"... ({len(lines) - preview_lines} more lines truncated) ...\")\n",
        "\n",
        "\n",
        "def get_repo_name(repo_url):\n",
        "    \"\"\"\n",
        "    Extract the repository name from the GitHub URL.\n",
        "    \"\"\"\n",
        "    return repo_url.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n",
        "\n",
        "# ===========================================\n",
        "# Main Execution\n",
        "# ===========================================\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # 1. Clone Repository\n",
        "        repo_name = get_repo_name(GITHUB_REPO_URL)\n",
        "        clone_path = os.path.join(SAVE_DIR, repo_name)\n",
        "        os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "        clone_repository(GITHUB_REPO_URL, clone_path)\n",
        "\n",
        "        # 2-6. Load or Build Data and Index\n",
        "        # This step handles extraction, embedding, building, and saving/loading\n",
        "        # of both functions data and the FAISS index.\n",
        "        print(\"\\nüì¶ Loading embedding model...\")\n",
        "        model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "        # Load or build functions data and FAISS index\n",
        "        functions, index = load_or_build_data_and_index(clone_path, model)\n",
        "\n",
        "        if not functions or index is None:\n",
        "             print(\"‚ùå Could not load or build data/index. Exiting.\")\n",
        "             exit(3)\n",
        "\n",
        "        # 7. Search\n",
        "        while True: # Loop for multiple searches until empty query is entered\n",
        "            query = input(\"\\nüí¨ Enter your search query (in English, or any language the embedding model handles well - press Enter only to quit): \")\n",
        "            if not query.strip():\n",
        "                print(\"üëã Exiting search.\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                results = search_functions(index, model, query, functions)\n",
        "                pretty_print_results(results)\n",
        "            except Exception as search_err:\n",
        "                print(f\"‚ùó An error occurred during search: {search_err}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùó Unexpected error occurred: {e}\")\n",
        "        exit(99)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ‚ùó „ÅîÊ≥®ÊÑè\n",
        "# „Åì„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„ÅØ„ÄåGoogle ColabÔºàL4 GPUÊé®Â•®Ôºâ„Äç„Åß„ÅÆÂÆüË°å„ÇíÊÉ≥ÂÆö„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n",
        "#\n",
        "# ‚ë† GitHub„É™„Éù„Ç∏„Éà„É™„Çí„ÇØ„É≠„Éº„É≥„Åó„ÄÅ\n",
        "# ‚ë° .py, .ipynb„Éï„Ç°„Ç§„É´„Åã„ÇâÈñ¢Êï∞„ÇíÊäΩÂá∫„ÅóÔºàÂàùÂõû„ÅÆ„ÅøÔºâ„ÄÅ\n",
        "# ‚ë¢ „Ç≥„Éº„Éâ„ÇíÂüã„ÇÅËæº„ÅøÔºàEmbeddingÔºâ„ÅóÔºàÂàùÂõû„ÅÆ„ÅøÔºâ„ÄÅ\n",
        "# ‚ë£ FAISS„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí‰ΩúÊàê„Åó„Å¶‰øùÂ≠ò„ÅóÔºàÂàùÂõû„ÅÆ„ÅøÔºâ„ÄÅ\n",
        "# ‚ë§ Êó•Êú¨Ë™û„ÇØ„Ç®„É™„ÇíQwen3-8B-FP8„ÅßËã±Ë®≥„Åó„Å¶„Åã„ÇâÊ§úÁ¥¢„Åó„Åæ„Åô„ÄÇ\n",
        "#\n",
        "# Êó•Êú¨Ë™û„ÅßË≥™Âïè„Åó„Å¶„ÇÇËã±Ë™û„Å´ÁøªË®≥„Åó„Å¶È´òÁ≤æÂ∫¶„Å´Ê§úÁ¥¢„Åß„Åç„Çã‰ªïÁµÑ„Åø„Åß„ÅôÔºÅ\n",
        "# ÂàùÂõûÂÆüË°åÊôÇ„ÇÑ„Ç≥„Éº„ÉâÊõ¥Êñ∞ÊôÇ„Å´„ÅØÊäΩÂá∫„ÉªÂüã„ÇÅËæº„Åø„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÊßãÁØâ„ÅåË°å„Çè„Çå„Åæ„Åô„Åå„ÄÅ\n",
        "# „Åù„Çå‰ª•Èôç„ÅØ‰øùÂ≠ò„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ„Åü„ÇÅÈ´òÈÄü„Å´Ê§úÁ¥¢„Åß„Åç„Åæ„ÅôÔºÅ\n",
        "# ===========================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import lizard\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ===========================================\n",
        "# Settings\n",
        "# ===========================================\n",
        "GITHUB_REPO_URL = \"https://github.com/google-research/bert.git\"  # üîß Change here\n",
        "SAVE_DIR = \"./cloned_repos\"\n",
        "MODEL_NAME = \"Shuu12121/CodeSearch-ModernBERT-Crow-Plus\"\n",
        "QWEN_MODEL = \"Qwen/Qwen3-8B-FP8\"\n",
        "MIN_FUNCTION_LENGTH = 3  # Minimum lines for function\n",
        "\n",
        "# „Éï„Ç°„Ç§„É´ÂêçÂÆöÁæ©\n",
        "FUNCTIONS_FILE = \"functions.json\"\n",
        "INDEX_FILE = \"faiss_index.bin\"\n",
        "\n",
        "# ===========================================\n",
        "# Helper Functions\n",
        "# ===========================================\n",
        "\n",
        "def clone_repository(repo_url, clone_dir):\n",
        "    if not os.path.exists(clone_dir):\n",
        "        subprocess.run([\"git\", \"clone\", repo_url, clone_dir], check=True)\n",
        "        print(f\"‚úÖ Repository cloned to {clone_dir}\")\n",
        "    else:\n",
        "        print(f\"‚ÑπÔ∏è Repository already exists at {clone_dir}. Skipping clone.\")\n",
        "\n",
        "def extract_functions(repo_path):\n",
        "    functions = []\n",
        "    print(\"üì• Extracting functions...\")\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        files.sort()\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            # .git„Å™„Å©„ÅÆÈö†„Åó„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇÑ‰∏çË¶Å„Å™„Éï„Ç°„Ç§„É´„ÅØ„Çπ„Ç≠„ÉÉ„Éó\n",
        "            if \".git\" in file_path or \".ipynb_checkpoints\" in file_path:\n",
        "                 continue\n",
        "            try:\n",
        "                if file.endswith(\".py\"):\n",
        "                    analysis = lizard.analyze_file(file_path)\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        lines = f.readlines()\n",
        "                    for func in analysis.function_list:\n",
        "                        if hasattr(func, 'start_line') and hasattr(func, 'end_line'):\n",
        "                            start, end = max(func.start_line - 1, 0), func.end_line\n",
        "                            code = \"\".join(lines[start:end]) # join„ÅßÊñáÂ≠óÂàó„Å´„Åô„Çã\n",
        "                            if len(code.strip().splitlines()) >= MIN_FUNCTION_LENGTH:\n",
        "                                functions.append({\n",
        "                                    \"file_path\": file_path,\n",
        "                                    \"function_name\": func.long_name,\n",
        "                                    \"code\": code\n",
        "                                })\n",
        "                elif file.endswith(\".ipynb\"):\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        data = json.load(f)\n",
        "                    for idx, cell in enumerate(data.get(\"cells\", [])):\n",
        "                        if cell.get(\"cell_type\") == \"code\":\n",
        "                            code = \"\".join(cell.get(\"source\", []))\n",
        "                            if len(code.strip().splitlines()) >= MIN_FUNCTION_LENGTH:\n",
        "                                functions.append({\n",
        "                                    \"file_path\": file_path,\n",
        "                                    \"function_name\": f\"cell_{idx}\",\n",
        "                                    \"code\": code\n",
        "                                })\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Warning: Could not process {file_path}: {e}\")\n",
        "    print(f\"‚úÖ Extracted {len(functions)} functions.\")\n",
        "    return functions\n",
        "\n",
        "def embed_codes(codes, model):\n",
        "    print(\"\\nüìà Encoding function codes...\")\n",
        "    return model.encode(codes, batch_size=32, show_progress_bar=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_faiss_index(embeddings):\n",
        "    print(\"\\nüèóÔ∏è Building FAISS index...\")\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def load_or_build_data_and_index(clone_path, model):\n",
        "    \"\"\"\n",
        "    Êó¢Â≠ò„ÅÆfunctions„Éá„Éº„Çø„Å®FAISS„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí„É≠„Éº„Éâ„Åô„Çã„Åã„ÄÅÁÑ°„Åë„Çå„Å∞Êñ∞„Åó„Åè‰ΩúÊàê„Åô„Çã„ÄÇ\n",
        "    \"\"\"\n",
        "    functions_path = os.path.join(clone_path, FUNCTIONS_FILE)\n",
        "    index_path = os.path.join(clone_path, INDEX_FILE)\n",
        "\n",
        "    # „Éá„Éº„Çø„Å®„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅÆ‰∏°Êñπ„ÅåÂ≠òÂú®„Åô„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ\n",
        "    if os.path.exists(functions_path) and os.path.exists(index_path):\n",
        "        print(f\"\\nüîÑ Loading existing data and index from {clone_path}...\")\n",
        "        try:\n",
        "            # functions„Éá„Éº„Çø„Çí„É≠„Éº„Éâ\n",
        "            with open(functions_path, 'r', encoding='utf-8') as f:\n",
        "                functions = json.load(f)\n",
        "            # FAISS„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí„É≠„Éº„Éâ\n",
        "            index = faiss.read_index(index_path)\n",
        "            print(\"‚úÖ Successfully loaded existing data and index.\")\n",
        "            return functions, index\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error loading existing data or index: {e}. Rebuilding...\")\n",
        "            # „É≠„Éº„Éâ„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØÂÜçÊßãÁØâ„Å∏ÈÄ≤„ÇÄ\n",
        "\n",
        "    # „Éá„Éº„Çø„Åæ„Åü„ÅØ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÄÅ„Åæ„Åü„ÅØ„É≠„Éº„Éâ„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà\n",
        "    print(f\"\\nüèóÔ∏è No existing data or index found (or failed to load). Building a new one...\")\n",
        "\n",
        "    # functions„ÇíÊäΩÂá∫\n",
        "    functions = extract_functions(clone_path)\n",
        "    if not functions:\n",
        "        print(\"‚ùå Error: No functions found. Cannot build index. Exiting.\")\n",
        "        return [], None # Á©∫„ÅÆ„É™„Çπ„Éà„Å®None„ÇíËøî„Åó„Å¶„É°„Ç§„É≥„Åß„Ç®„É©„ÉºÂá¶ÁêÜ„Åï„Åõ„Çã\n",
        "\n",
        "    # functions„Éá„Éº„Çø„Çí‰øùÂ≠ò\n",
        "    try:\n",
        "        with open(functions_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(functions, f, indent=4)\n",
        "        print(f\"üíæ Functions data saved at: {functions_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not save functions data to {functions_path}: {e}\")\n",
        "\n",
        "    # „Ç≥„Éº„Éâ„ÇíÂüã„ÇÅËæº„Åø\n",
        "    codes = [func[\"code\"] for func in functions]\n",
        "    embeddings = embed_codes(codes, model)\n",
        "\n",
        "    # FAISS„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÊßãÁØâ\n",
        "    index = build_faiss_index(embeddings)\n",
        "\n",
        "    # FAISS„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí‰øùÂ≠ò\n",
        "    try:\n",
        "        faiss.write_index(index, index_path)\n",
        "        print(f\"üíæ FAISS index saved at: {index_path}\")\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Warning: Could not save FAISS index to {index_path}: {e}\")\n",
        "\n",
        "\n",
        "    return functions, index\n",
        "\n",
        "\n",
        "def search_functions(index, model, query, functions, top_k=5):\n",
        "    query_emb = model.encode([query], device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    D, I = index.search(np.array(query_emb), top_k)\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "         if 0 <= idx < len(functions): # Âøµ„ÅÆ„Åü„ÇÅ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅÆÁØÑÂõ≤„ÉÅ„Çß„ÉÉ„ÇØ\n",
        "              results.append(functions[idx])\n",
        "         else:\n",
        "              print(f\"‚ö†Ô∏è Warning: Invalid index {idx} returned from FAISS search.\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def pretty_print_results(results):\n",
        "    print(\"\\nüîç Search Results:\")\n",
        "    if not results:\n",
        "        print(\"No relevant functions found.\")\n",
        "        return\n",
        "    for idx, res in enumerate(results, start=1):\n",
        "        print(f\"\\n=== Result {idx} ===\")\n",
        "        print(f\"üìÑ File: {res['file_path']}\")\n",
        "        print(f\"üîß Function: {res['function_name']}\")\n",
        "        print(f\"üß© Code Preview:\")\n",
        "        lines = res['code'].splitlines()\n",
        "        # „Ç≥„Éº„Éâ„ÅåÈï∑„Åô„Åé„ÇãÂ†¥Âêà„ÅØ‰∏ÄÈÉ®„Å†„ÅëË°®Á§∫\n",
        "        preview_lines = 100\n",
        "        for line in lines[:preview_lines]:\n",
        "            print(line)\n",
        "        if len(lines) > preview_lines:\n",
        "            print(f\"... ({len(lines) - preview_lines} more lines truncated) ...\")\n",
        "\n",
        "def translate_to_english(qwen_model, qwen_tokenizer, japanese_text):\n",
        "    \"\"\"\n",
        "    Qwen3-8B-FP8„Çí‰Ωø„Å£„Å¶„ÄÅÊäÄË°ìÊñáÊõ∏Âêë„Åë„Å´Ëá™ÁÑ∂„Å™Ëã±Ë™û„Å∏ÁøªË®≥„Åô„Çã„ÄÇ\n",
        "    \"\"\"\n",
        "    prompt_translate = f\"\"\"\n",
        "    ‰ª•‰∏ã„ÅÆÊó•Êú¨Ë™û„ÅÆÂÜÖÂÆπ„Çí„ÄÅËá™ÁÑ∂„Å™Ëã±Ë™û„Å´ÁøªË®≥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
        "    „ÉªÂ∞ÇÈñÄÁî®Ë™û„ÇÑ„Ç≥„Éº„Éâ„ÅÆÂ§âÊï∞Âêç„ÅØ„Åù„ÅÆ„Åæ„Åæ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
        "    „ÉªÊ≠£Á¢∫„Åã„Å§Ëá™ÁÑ∂„Å™Ëã±Ë™û„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
        "    „ÉªÁøªË®≥ÂØæË±°:\n",
        "    ---\n",
        "    {japanese_text}\n",
        "    ---\n",
        "    Ëã±Ë®≥:\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_translate.strip()}]\n",
        "\n",
        "    # Êé®Ë´ñË®≠ÂÆö„ÇíÊòéÁ§∫ÁöÑ„Å´ÊåáÂÆö\n",
        "    generation_config = {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 5,\n",
        "        \"min_p\": 0,\n",
        "        \"pad_token_id\": qwen_tokenizer.eos_token_id,\n",
        "        \"eos_token_id\": qwen_tokenizer.eos_token_id\n",
        "    }\n",
        "\n",
        "    text = qwen_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False # Á∞°ÊΩî„Å´ÁøªË®≥„Å†„Åë„Åï„Åõ„Çã\n",
        "    )\n",
        "\n",
        "    inputs = qwen_tokenizer([text], return_tensors=\"pt\").to(qwen_model.device)\n",
        "\n",
        "    generated_ids = qwen_model.generate(\n",
        "        **inputs,\n",
        "        **generation_config # Ë®≠ÂÆö„ÇíÊ∏°„Åô\n",
        "    )\n",
        "\n",
        "    # ÂÖ•ÂäõÈÉ®ÂàÜ„ÇíÈô§Âéª„Åó„ÄÅ„É™„Çπ„Éà„Å´Â§âÊèõ\n",
        "    output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
        "\n",
        "    # EOS„Éà„Éº„ÇØ„É≥„ÅßÂàáÊñ≠\n",
        "    try:\n",
        "        # Qwen„É¢„Éá„É´„ÅØË§áÊï∞„ÅÆEOS„Éà„Éº„ÇØ„É≥„ÇíÊåÅ„Å§Â†¥Âêà„Åå„ÅÇ„Çã„ÅÆ„Åß„ÄÅ„É™„Çπ„Éà„ÅßÊåáÂÆö„Åô„Çã\n",
        "        # „Åæ„Åü„ÅØtokenizer.eos_token_id„Åå„É™„Çπ„Éà„ÅÆÂ†¥Âêà„ÅØ„Åù„Çå„Çí‰Ωø„ÅÜ\n",
        "        eos_ids = qwen_tokenizer.eos_token_id\n",
        "        if not isinstance(eos_ids, list):\n",
        "             eos_ids = [eos_ids]\n",
        "\n",
        "        min_eos_index = len(output_ids)\n",
        "        for eos_id in eos_ids:\n",
        "             try:\n",
        "                  idx = output_ids.index(eos_id)\n",
        "                  min_eos_index = min(min_eos_index, idx)\n",
        "             except ValueError:\n",
        "                  pass # EOS„Éà„Éº„ÇØ„É≥„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØÁ∂öË°å\n",
        "\n",
        "        output_ids = output_ids[:min_eos_index]\n",
        "\n",
        "    except Exception as e:\n",
        "        # ‰∏á„Åå‰∏Ä„ÅÆ‰æãÂ§ñÊôÇ„ÇÇ„Éá„Ç≥„Éº„Éâ„ÇíË©¶„Åø„Çã\n",
        "        print(f\"‚ö†Ô∏è Warning during EOS token handling: {e}\")\n",
        "        pass\n",
        "\n",
        "\n",
        "    translated_text = qwen_tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Qwen„ÅÆÂá∫Âäõ„ÅÆÊúÄÂæå„Å´„Åü„Åæ„Å´‰∏çË¶Å„Å™ÊñáÂ≠ó„Åå„Å§„ÅèÂ†¥Âêà„Åå„ÅÇ„Çã„ÅÆ„Åß„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó\n",
        "    # ‰æã: <|im_end|> „ÇÑ„Åù„Çå„Å´È°û„Åô„Çã„ÇÇ„ÅÆ\n",
        "    # skip_special_tokens=True „ÅßÂ§ßÊäµ„ÅØÈô§Âéª„Åï„Çå„Åæ„Åô„Åå„ÄÅÂøµ„ÅÆ„Åü„ÇÅ\n",
        "    # Qwen„ÅÆÁâπÂÆö„ÅÆÂá∫ÂäõÂΩ¢Âºè„Å´Âêà„Çè„Åõ„Å¶Ë™øÊï¥„ÅåÂøÖË¶Å„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì\n",
        "    # „Åì„Åì„Åß„ÅØ‰∏ÄËà¨ÁöÑ„Å™„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„ÅØË°å„Çè„Åö„ÄÅdecode„ÅÆÁµêÊûú„Çí‰ø°È†º„Åó„Åæ„Åô„ÄÇ\n",
        "    # ÂøÖË¶Å„Åß„ÅÇ„Çå„Å∞ translated_text = translated_text.split('<|im_end|>')[0].strip() „Å™„Å©„ÇíËøΩÂä†\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "def get_repo_name(repo_url):\n",
        "    return repo_url.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n",
        "\n",
        "# ===========================================\n",
        "# Main Execution\n",
        "# ===========================================\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # 1. Clone repository\n",
        "        repo_name = get_repo_name(GITHUB_REPO_URL)\n",
        "        clone_path = os.path.join(SAVE_DIR, repo_name)\n",
        "        os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "        clone_repository(GITHUB_REPO_URL, clone_path)\n",
        "\n",
        "        # 2-6. Load or Build Data and Index\n",
        "        # „Åì„Åì„Åßfunctions„ÅÆÊäΩÂá∫„ÄÅÂüã„ÇÅËæº„Åø„ÄÅ„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÊßãÁØâ„Éª‰øùÂ≠ò„ÄÅ„Åæ„Åü„ÅØË™≠„ÅøËæº„Åø„ÅåË°å„Çè„Çå„Çã\n",
        "        print(\"\\nüì¶ Loading embedding model...\")\n",
        "        model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "        print(\"\\nüì¶ Loading translation model (Qwen3-8B-FP8)...\")\n",
        "        qwen_tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL, trust_remote_code=True)\n",
        "        # device_map=\"auto\"„Çí‰ΩøÁî®„Åô„Çã„Å®„É¢„Éá„É´„ÅåËá™ÂãïÁöÑ„Å´„Éá„Éê„Ç§„Çπ„Å´ÈÖçÁΩÆ„Åï„Çå„Çã\n",
        "        qwen_model = AutoModelForCausalLM.from_pretrained(QWEN_MODEL, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "\n",
        "        # functions„Éá„Éº„Çø„Å®FAISS„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí„É≠„Éº„Éâ„Åæ„Åü„ÅØÊñ∞„Åó„ÅèÊßãÁØâ\n",
        "        functions, index = load_or_build_data_and_index(clone_path, model)\n",
        "\n",
        "        if not functions or index is None:\n",
        "             print(\"‚ùå Could not load or build data/index. Exiting.\")\n",
        "             exit(3)\n",
        "\n",
        "        # 7. Search\n",
        "        while True: # „É¶„Éº„Ç∂„Éº„ÅåÁ©∫Ë°å„ÇíÂÖ•Âäõ„Åô„Çã„Åæ„ÅßÊ§úÁ¥¢„ÇíÁπ∞„ÇäËøî„Åô„É´„Éº„Éó\n",
        "            japanese_query = input(\"\\nüí¨ Êó•Êú¨Ë™û„ÅßÊ§úÁ¥¢„ÇØ„Ç®„É™„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ (ÁµÇ‰∫Ü„Åô„Çã„Å´„ÅØEnter„Ç≠„Éº„ÅÆ„Åø„ÇíÊäº„Åô): \")\n",
        "            if not japanese_query.strip():\n",
        "                print(\"üëã Ê§úÁ¥¢„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åô„ÄÇ\")\n",
        "                break\n",
        "\n",
        "            print(\"\\nüîÑ Translating query to English...\")\n",
        "            try:\n",
        "                english_query = translate_to_english(qwen_model, qwen_tokenizer, japanese_query)\n",
        "                print(f\"üåé English Query: {english_query}\")\n",
        "\n",
        "                results = search_functions(index, model, english_query, functions)\n",
        "                pretty_print_results(results)\n",
        "            except Exception as search_err:\n",
        "                print(f\"‚ùó An error occurred during search or translation: {search_err}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùó Unexpected error occurred: {e}\")\n",
        "        exit(99)"
      ],
      "metadata": {
        "id": "0h2WY-i9gYbL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
